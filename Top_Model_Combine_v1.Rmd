---
title: "Combine"
author: "Any"
date: "2/4/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r results='hide', message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(scales)
library(textstem)

#upload the data into shiny
cirque <- read_csv("Data/Cirque_Train.csv")
#med <- read_csv("Ghislene/Data/drug_combine.csv")
```

```{r results='hide', message=FALSE, warning=FALSE}
## SHINY INPUT VALUES
#match the variables with column names under Shiny
corpus <- cirque$Text
response <- cirque$Show
#ratings <- 

#corpus <- med$comment
#response <- med$drug_name
#ratings <- med ratings

#Shiny other input values
#n_grams can be either 1 or 2
n_grams = 2

#optimize K as a suggestion or allow users to choose K number
k_value = 10

#allow users to pick the percentage of words used in the document (suggest 50,60,70 percentages)
#remove words that per repeated more than those percentage
repeatPerc = .50
```

```{r, warning=FALSE}
#create train model
#include ratings later
model <- tibble::tibble(corpus, response)

#Unigram and remove numbers and punctuations
clean_model <- model %>%
  mutate(id = row_number()) %>%
  group_by(response) %>%
  unnest_tokens(word, corpus, token = "ngrams", n = n_grams) %>%
  ungroup()

clean_model$word <- gsub('[[:punct:]]+', 'NA', clean_model$word)
clean_model$word <- gsub('[[:digit:]]+', 'NA', clean_model$word)

#Put titles words into a dataframe
titleWords <- tibble(omit = unique(clean_model$response))
titleWords <- titleWords %>%
  unnest_tokens(word, omit) %>%
  distinct()

if (n_grams == 1){
  #root form
  clean_model <- clean_model %>%
    mutate(word = lemmatize_words(word))
  
  #Remove stop words and title words and NA words
  clean_model <- clean_model %>%
    filter(!word %in% stop_words$word, !word %in% titleWords$word, !word %in% "NA") %>%
    ungroup()
  
  #the percentage of words that were repeated
  wordCount <- clean_model %>%
    count(word,sort = TRUE) %>%
    mutate(perc = n / sum(n) * 100, value = if_else(perc > repeatPerc, "TRUE", "FALSE")) %>%
    filter(value == "TRUE")

  #remove the repeated words
  clean_model <- clean_model %>%
    filter(!word %in% wordCount$word) %>%
    mutate(ind = row_number())
}

if (n_grams == 2){
  #root form
  clean_model <- clean_model %>%
    separate(word, c("word1", "word2"), sep = " ", remove = F) %>%
    mutate(word1 = lemmatize_words(word1), word2 = lemmatize_words(word2))
  
  #Remove stop words and title words and NA words
  clean_model <- clean_model %>%
    filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word, !word1 %in% titleWords$word, !word2 %in% titleWords$word, !word1 %in% "NA", !word2 %in% "NA") %>%
    mutate(condition = if_else(word1 == word2, "TRUE", "FALSE")) %>%
    filter(condition == "FALSE") %>%
    ungroup()
  
  #the percentage of words that were repeated
  wordCount <- clean_model %>%
    count(word,sort = TRUE) %>%
    mutate(perc = n / sum(n) * 100, value = if_else(perc > repeatPerc, "TRUE", "FALSE")) %>%
    filter(value == "TRUE")
  
  #remove the repeated words
  clean_model <- clean_model %>%
    filter(!word %in% wordCount$word) %>%
    mutate(ind = row_number())
}

```

```{r, warning=FALSE}
#Shiny Output
#show the user the repeated words that will be removed based on this percentage criteria
wordCount
```

```{r, warning=FALSE}
#bring the individual words back into a text
tokens <- clean_model %>% 
  select(id, word, response) %>%
  group_by(id) %>%
  mutate(ind = row_number()) %>%
  spread(key = ind, value = word)

tokens [is.na(tokens)] <- ""
tokens <- unite(tokens, text,-id, -response, sep =" " )
```

```{r, message=FALSE}
#create DTM
#we can do this using Tidytext and Tm
#unnest_tokens again causes errors after data cleaning
#prueba <- tokens %>%
#  unnest_tokens(word, text, token = "ngrams", n = n_grams) %>%
#  count(id,word)

prueba <- clean_model %>%
  select(id, word) %>%
  count(id,word)

clean_model %>%
  count(word,sort = TRUE)

dtm_prueba <- prueba %>%
  cast_dtm(id, word, n)


tokens %>%
  ungroup() %>%
    select(text) %>%
  unnest_tokens(word,text) %>%
  count(word,sort = TRUE) %>%
  top_n(100)
  
#prueba %>%
#  cast_dfm(id, word, n)


##explore the basic frequency
#tf <- TermDocFreq(dtm = dtm)
#original_tf <- tf %>% select(term, term_freq,doc_freq)
#rownames(original_tf) <- 1:nrow(original_tf)
## Eliminate words appearing less than 2 times or in more than half of the
## documents
#vocabulary <- tf$term[ tf$term_freq > 1 & tf$doc_freq < nrow(dtm) / 2 ]
#dtm = dtm
#

library(topicmodels)

ap_lda <- LDA(dtm_prueba, k = k_value, control = list(seed = 1234))
ap_lda

library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics


library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(25, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()



beta_spread <- ap_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_spread


beta_spread %>%
  top_n(25) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_reordered()

```
