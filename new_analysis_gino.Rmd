---
title: "GS_new_models"
author: "Gino Slanzi"
date: "2/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r results='hide', message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(scales)
library(textstem)

#upload the data into shiny
cirque <- read_csv("Data/Cirque_Train.csv")
#med <- read_csv("Ghislene/Data/drug_combine.csv")
```

```{r results='hide', message=FALSE, warning=FALSE}
## SHINY INPUT VALUES
#match the variables with column names under Shiny
corpus <- cirque$Text
response <- cirque$Show
#ratings <- 

#corpus <- med$comment
#response <- med$drug_name
#ratings <- med ratings

#Shiny other input values
#n_grams can be either 1 or 2
n_grams = 2

#optimize K as a suggestion or allow users to choose K number
k_value = 10

#allow users to pick the percentage of words used in the document (suggest 50,60,70 percentages)
#remove words that per repeated more than those percentage
repeatPerc = .50
```


```{r, warning=FALSE}
#create train model
#include ratings later
model <- tibble::tibble(corpus, response)

#Unigram or Bigram according to the user's selection and remove numbers and punctuations
clean_model <- model %>%
  mutate(id = row_number()) %>%
  group_by(response) %>%
  unnest_tokens(word, corpus, token = "ngrams", n = n_grams) %>%
  ungroup()

clean_model$word <- gsub('[[:punct:]]+', 'NA', clean_model$word)
clean_model$word <- gsub('[[:digit:]]+', 'NA', clean_model$word)

table(model$response)

#Put titles words into a dataframe
titleWords <- tibble(omit = unique(clean_model$response))
titleWords <- titleWords %>%
  unnest_tokens(word, omit) %>%
  distinct()

clean_model <- clean_model %>%
    separate(word, c("word1", "word2"), sep = " ", remove = F) %>%
    mutate(word1 = lemmatize_words(word1), word2 = lemmatize_words(word2))

#Remove stop words and title words and NA words
clean_model <- clean_model %>%
    filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word, !word1 %in% titleWords$word, !word2 %in% titleWords$word, !word1 %in% "NA", !word2 %in% "NA") %>%
    mutate(condition = if_else(word1 == word2, "TRUE", "FALSE")) %>%
    filter(condition == "FALSE") %>%
    ungroup()

clean_model$word = paste0(clean_model$word1, " ", clean_model$word2)

```

```{r}
#the percentage of words that were repeated

wordCount <- clean_model %>%
  group_by(word) %>%
  mutate(count_w = n()) %>%
  arrange(desc(count_w))

#book_words <- clean_model %>%
#  count(id, word,sort = TRUE) %>%
#  bind_tf_idf(word, id, n) %>%
#  arrange(tf_idf)


#wordCount <- clean_model %>%
#  group_by(word) %>%
#  mutate(count_w = n()) %>%
#  arrange(desc(count_w))
  
#  mutate(count_words = n(word)) %>%
#  mutate(percentage = count_words/sum(count_words)* 100, value = if_else(percentage > repeatPerc, "TRUE", "FALSE"))

#  wordCount <- clean_model %>%
#    count(word,sort = TRUE) %>%
#    mutate(n2 = n, sum_n = sum(n), perc = (n2/sum_n)* 100, value = if_else(perc > repeatPerc, "TRUE", "FALSE"))

```

```{r}
tokens <- clean_model %>% 
  select(id, word, response) %>%
  group_by(id) %>%
  mutate(ind = row_number()) %>%
  spread(key = ind, value = word)

tokens [is.na(tokens)] <- ""
tokens <- unite(tokens, text,-id, -response, sep =" " )
```

```{r}
prueba <- clean_model %>%
  select(id, word) %>%
  count(id,word)

clean_model %>%
  count(word,sort = TRUE)

dtm_prueba <- prueba %>%
  cast_dtm(id, word, n)
```

```{r}
library(topicmodels)

ap_lda <- LDA(dtm_prueba, k = k_value, control = list(seed = 1234))
ap_lda

library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics

```

```{r}
library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(25, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```

```{r}

## TD-IDF
cirqueTFIDF <- clean_model %>%
  count(id, word, sort = TRUE) %>%
  bind_tf_idf(word, id, n) %>%
  group_by(id) %>%
  arrange(desc(tf_idf)) %>%
  ungroup() %>%
  mutate(xOrder=n():1)
```


Eliminate words with low TF-IDF score:
```{r}
eliminar <- cirqueTFIDF %>%
  top_n(-30) %>%
  arrange(tf_idf) %>%
  select(word)

eliminar <- unique(eliminar$word)

```

Try topic model again without common words:

```{r}
prueba2 <- clean_model %>%
  filter(!(word %in% eliminar)) %>%
  select(id, word) %>%
  count(id,word)

dtm_prueba2 <- prueba2 %>%
  cast_dtm(id, word, n)

ap_lda2 <- LDA(dtm_prueba2, k = k_value, control = list(seed = 1234))

ap_topics2 <- tidy(ap_lda2, matrix = "beta")

ap_top_terms2 <- ap_topics2 %>%
  group_by(topic) %>%
  top_n(25, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms2 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```




