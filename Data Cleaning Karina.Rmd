
```{r results='hide', message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(scales)
library(textstem)

#upload the data into shiny
cirque <- read_csv("Data/Cirque_Train.csv")
#med <- read_csv("Ghislene/Data/drug_combine.csv")
```

```{r results='hide', message=FALSE, warning=FALSE}
## SHINY INPUT VALUES
#match the variables with column names under Shiny
corpus <- cirque$Text
response <- cirque$Show
#ratings <- 


#corpus <- med$comment
#response <- med$drug_name
#ratings <- med ratings

#Shiny other input values
#n_grams can be either 1 or 2
n_grams = 2

#optimize K as a suggestion or allow users to choose K number
k_value = 10

#allow users to pick the percentage of words used in the document (suggest 50,60,70 percentages)
#remove words that per repeated more than those percentage
repeatPerc = .50
```

```{r, warning=FALSE}
#create train model
#include ratings later
model <- tibble::tibble(corpus, response)

#Unigram and remove numbers and punctuations
clean_model <- model %>%
  mutate(id = row_number()) %>%
  group_by(response) %>%
  unnest_tokens(word, corpus, token = "ngrams", n = n_grams) %>%
  ungroup()

clean_model$word <- gsub('[[:punct:]]+', 'NA', clean_model$word)
clean_model$word <- gsub('[[:digit:]]+', 'NA', clean_model$word)

#Put titles words into a dataframe
titleWords <- tibble(omit = unique(clean_model$response))
titleWords <- titleWords %>%
  unnest_tokens(word, omit) %>%
  distinct()

if (n_grams == 1){
  #root form
  clean_model <- clean_model %>%
    mutate(word = lemmatize_words(word))
  
  #Remove stop words and title words and NA words
  clean_model <- clean_model %>%
    filter(!word %in% stop_words$word, !word %in% titleWords$word, !word %in% "NA") %>%
    ungroup()
  
  #the percentage of words that were repeated
  wordCount <- clean_model %>%
    count(word,sort = TRUE) %>%
    mutate(perc = (n / sum(n)) * 100, value = if_else(perc > repeatPerc, "TRUE", "FALSE"))
  
#  #remove the repeated words
#  clean_model <- clean_model %>%
#    filter(!word %in% wordCount$word) %>%
#    mutate(ind = row_number())
}

if (n_grams == 2){
  #root form
  clean_model <- clean_model %>%
    separate(word, c("word1", "word2"), sep = " ", remove = F) %>%
    mutate(word1 = lemmatize_words(word1), word2 = lemmatize_words(word2))
  
  #Remove stop words and title words and NA words
  clean_model <- clean_model %>%
    filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word, !word1 %in% titleWords$word, !word2 %in% titleWords$word, !word1 %in% "NA", !word2 %in% "NA") %>%
    mutate(condition = if_else(word1 == word2, "TRUE", "FALSE")) %>%
    filter(condition == "FALSE") %>%
    ungroup()
  
  #the percentage of words that were repeated
  wordCount <- clean_model %>%
    count(word,sort = TRUE) %>%
    mutate(perc = (n / sum(n)) * 100, value = if_else(perc > repeatPerc, "TRUE", "FALSE"))
  
#  #remove the repeated words
#  clean_model <- clean_model %>%
#    filter(!word %in% wordCount$word) %>%
#    mutate(ind = row_number())
}

```

```{r, warning=FALSE}
#Shiny Output
#show the user the repeated words that will be removed based on this percentage criteria
wordCount
```

```{r, warning=FALSE}
#bring the individual words back into a text
tokens <- clean_model %>% 
  select(id, word, response) %>%
  group_by(id) %>%
  mutate(ind = row_number()) %>%
  spread(key = ind, value = word)

tokens [is.na(tokens)] <- ""
tokens <- unite(tokens, text,-id, -response, sep =" " )
```

```{r, warning=FALSE}
write.csv(tokens,"Data/Tokens_Clean.csv", row.names = FALSE)
```
