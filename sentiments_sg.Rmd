---
title: "sentiment"
author: "Team Baby Yoda"
date: "2/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

``` {r}

# packages

#install.packages("wordcloud")
#install.packages("textdata") 
#install.packages("syuzhet")
#install.packages('sentimentr')
#install.packages('ggridges')

library(tidyverse)
library(tidytext)
library(wordcloud)
library(tm)
library(syuzhet)
library(textdata)
library(sentimentr)
install.packages("ggridges")
library(ggridges)

```

``` {r}
#sentiments
bing <- get_sentiments("bing") 
#afinn <- get_sentiments("afinn")
nrc <- get_sentiments("nrc")

```


``` {r}

# #Already did this once, don't need to do again.
# 
# # read data
# c <- read_csv("Data/Cirque_Train.csv")
# 
# 
# 
# # clean the text
# c$Text <- gsub("&#39;", "", c$Text)
# c$Text <- gsub("&#34;", "", c$Text)
# c$Show <- gsub("Cirque du Soleil - ", "", c$Show)
# 
# 
# # save dataset again with cleaned text
# write_csv(c, "Data/cirque_du_soleil_clean.csv")

```


```{r}

# read in cleaned set
c <- read_csv("Data/cirque_du_soleil_clean.csv")
tokens <- read_csv("Data/Tokens_Clean.csv")

```


# Word Cloud
``` {r}


###### CONFIGURATION PAGE WORDCLOUD

# Word Cloud

# for the original dataset - original reviews as they are
# word counts
word_counts <- c %>%
  unnest_tokens(word, Text) %>%
  count(word) %>%
  anti_join(stop_words)

#get top 100 words in all shows combined
words_top100 <- word_counts %>%
  top_n(100)

#word cloud
wordcloud(words_top100$word,
           words_top100$n,
           scale=c(2,0.5), 
           colors=brewer.pal(8,"Dark2"))


#------

# for the Tokens_Clean csv
# word counts
word_counts_T <- tokens %>%
  unnest_tokens(word, text) %>%
  count(word) %>%
  anti_join(stop_words)

#get top 100 words in all shows combined
tokens_top100 <- word_counts_T %>%
  top_n(100)

#word cloud
wordcloud(tokens_top100$word,
           words_top100$n,
           scale=c(2,0.5), 
           colors=brewer.pal(8,"Dark2"))

```



# Sentiment comparison word cloud for the original dataset- original reviews as they are
``` {r}
c_t <- c$Text

s_c <- get_nrc_sentiment(c_t)
sent_c <- colSums(s_c)
sent_sum_c <- data.frame(count = sent_c, sentiment = names(sent_c))
sent_sum_c$sentiment= factor(sent_sum_c$sentiment, levels = sent_sum_c$sentiment[order(sent_sum_c$count, decreasing = TRUE)])

#this below didn't work. keep scrolling for a working version
# wc_sents <- c(
#   paste(c_t[s_c$anger>0], collapse = " "),
#   paste(c_t[s_c$anticipation>0], collapse = " "),
#   paste(c_t[s_c$disgust>0], collapse = " "),
#   paste(c_t[s_c$fear>0], collapse = " "),
#   paste(c_t[s_c$joy>0], collapse = " "),
#   paste(c_t[s_c$sadness>0], collapse = " "),
#   paste(c_t[s_c$surprise>0], collapse = " "),
#   paste(c_t[s_c$trust>0], collapse = " "),
#   #paste(c_t[s_c$negative>0], collapse = " "),
#   #paste(c_t[s_c$positive>0], collapse = " "),
# )

ang_c <- paste(c_t[s_c$anger>0], collapse = " ")
ant_c <- paste(c_t[s_c$anticipation>0], collapse = " ")
disg_c <- paste(c_t[s_c$disgust>0], collapse = " ")
fear_c <- paste(c_t[s_c$fear>0], collapse = " ")
joy_c <- paste(c_t[s_c$joy>0], collapse = " ")
sad_c <- paste(c_t[s_c$sadness>0], collapse = " ")
surp_c <- paste(c_t[s_c$surprise>0], collapse = " ")
trust_c <- paste(c_t[s_c$trust>0], collapse = " ")

wc_sents_c <- c(ang_c, ant_c, disg_c, fear_c, joy_c, sad_c, surp_c, trust_c)

wc_corpus_c <- Corpus(VectorSource(wc_sents_c))

wc_tdm_c <- TermDocumentMatrix(wc_corpus_c)
wc_tdm_c <- as.matrix(wc_tdm_c)

colnames(wc_tdm_c) <- c('anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust')

comparison.cloud(wc_tdm_c, random.order = TRUE, 
                 colors = brewer.pal(8,"Dark2"),
                 title.size = 0.75, max.words = 250,
                 scale = c(1.5, 0.4), rot.per = 0.325)
```


# Sentiment Comparison Word Cloud - for the tokenized dataset
``` {r}

#sentiment comparison word cloud

#done for the tokenized data
t_t <- tokens$text

s <- get_nrc_sentiment(t_t)
sent_ <- colSums(s)
sent_sum <- data.frame(count = sent_, sentiment = names(sent_))
sent_sum$sentiment= factor(sent_sum$sentiment, levels = sent_sum$sentiment[order(sent_sum$count, decreasing = TRUE)])

#this below didn't work. keep scrolling for a working version
# wc_sents <- c(
#   paste(t_t[s$anger>0], collapse = " "),
#   paste(t_t[s$anticipation>0], collapse = " "),
#   paste(t_t[s$disgust>0], collapse = " "),
#   paste(t_t[s$fear>0], collapse = " "),
#   paste(t_t[s$joy>0], collapse = " "),
#   paste(t_t[s$sadness>0], collapse = " "),
#   paste(t_t[s$surprise>0], collapse = " "),
#   paste(t_t[s$trust>0], collapse = " "),
#   #paste(t_t[s$negative>0], collapse = " "),
#   #paste(t_t[s$positive>0], collapse = " "),
# )

ang <- paste(t_t[s$anger>0], collapse = " ")
ant <- paste(t_t[s$anticipation>0], collapse = " ")
disg <- paste(t_t[s$disgust>0], collapse = " ")
fear <- paste(t_t[s$fear>0], collapse = " ")
joy <- paste(t_t[s$joy>0], collapse = " ")
sad <- paste(t_t[s$sadness>0], collapse = " ")
surp <- paste(t_t[s$surprise>0], collapse = " ")
trust <- paste(t_t[s$trust>0], collapse = " ")

wc_sents <- c(ang, ant, disg, fear, joy, sad, surp, trust)

wc_corpus <- Corpus(VectorSource(wc_sents))

wc_tdm <- TermDocumentMatrix(wc_corpus)
wc_tdm <- as.matrix(wc_tdm)

colnames(wc_tdm) <- c('anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust')

comparison.cloud(wc_tdm, random.order = TRUE, 
                 colors = brewer.pal(8,"Dark2"),
                 title.size = 1,
                 max.words = 30,
                 scale = c(1, 1), 
                 rot.per = 0.1)

```




# Sentiments

## NRC
``` {r}

# sentiment 

# nrc sentiments

sentiment.orientation <- data.frame(orientation = c(rep("Positive",5),rep("Negative",5)),
                                sentiment = c("anticipation","joy","positive","trust","surprise","anger","disgust","fear","negative","sadness"))


# nrc sentiment grouped by show/response ---

nrc_sent <- c %>%
  unnest_tokens(word, Text) %>%
  count(Show,word) %>%
  group_by(Show) %>%
  inner_join(nrc ,by=c("word")) %>%
  group_by(Show,sentiment) %>%
  summarize(total=sum(n))

#graph - with facetwrap
nrc_sent_barg <- nrc_sent %>%
  ggplot(aes(x=sentiment,y=total,fill=sentiment, group = Show)) + geom_bar(stat='identity',position='dodge')+ 
  facet_wrap(~Show, scales='free') +
  ylab('Word Count') +
  xlab('Sentiment') +
  labs(title='Sentiment Frequency per Show') + 
  theme(axis.text.x  = element_text(angle=90))

nrc_sent_barg


#graph - without facetwrap

response_groups <- levels(factor(c$Show))
sentiment_per_response_list <- list()
for (i in seq_along(response_groups)){
  sentiment_per_response <-nrc_sent %>%
  filter(Show == response_groups[i]) %>% ggplot(aes(x=reorder(sentiment,total),y=total)) + geom_bar(stat='identity',position='dodge') +   ylab('Word Count') +
  xlab('Sentiment') +
  #labs(title=paste('Sentiment Frequency for', r)) + 
  theme(axis.text.x  = element_text(angle=45))
  sentiment_per_response_list[[i]] <- sentiment_per_response
}





ggpubr::ggarrange(plotlist = (sentiment_per_response_list), ncol=1, nrow = length(sentiment_per_response_list))

sentiment_freq_per_response <- grid.arrange(
    grobs = (sentiment_per_response_list),
    nrow = 5,
    ncol = 1,
    top = "Sentiment Frequency per Response",
    heights=c(0.2,1,1,1,1)
  )

#-------

# not grouped - for the whole dataset
nrc_sent_all <- c %>%
  unnest_tokens(word, Text) %>%
  count(word) %>%
  inner_join(nrc ,by=c("word")) %>%
  group_by(sentiment) %>%
  summarize(total=sum(n))

#graph
nrc_sent_barg_all <- nrc_sent_all %>%
  ggplot(aes(x=reorder(sentiment,total),y=total,fill=sentiment)) + geom_bar(stat='identity',position='dodge')+ 
  ylab('Word Count') +
  xlab('Sentiment') +
  labs(title='Sentiment Frequency') + 
  theme(axis.text.x  = element_text(angle=45))

nrc_sent_barg_all




```

## SentimentR

``` {r}

#sentimentr 

#gives the mean and standard deviation of sentiments by show/response

sentr_sent <- c %>%
  get_sentences() %>%
  sentiment_by(by = 'Show')

sentr_barg <- ggplot(sentr_sent, aes(x = reorder(Show, ave_sentiment), y = ave_sentiment, color = Show)) + geom_point(size=5,show.legend = F) + labs(x = 'Show', y = 'Average Sentiment', title = "Average Sentiment per Show")

sentr_barg

#-----

#ggridges plot for sentiment distribution per response group
c$num <- seq.int(nrow(c))

sentr_byrev <- c %>%
  get_sentences() %>%
  sentiment_by(by = 'num') %>%
  left_join(select(c,num,Show))

#ggridges plot
sent_dist_plot <- sentr_byrev %>%
  mutate(r_levels=factor(Show)) %>% 
  ggplot(aes(x = ave_sentiment, y = r_levels, group = r_levels,fill=r_levels)) +
  geom_density_ridges(scale = 2.0, size = 0.25,alpha=0.4,show.legend=F) +
  scale_x_continuous(limits=c(-.2, 0.8), expand = c(0.01, 0)) +
  theme_bw() + 
  geom_vline(aes(xintercept=0)) + 
  labs(x = 'Sentiment', y = 'Response Group', title = 'Distribution of Sentiment by Response Group')

sent_dist_plot

#------


#highlight - it doesn't work :(
sentr_highlight <- c %>%
  select(Text) %>%
  get_sentences() %>%
  sentiment_by(averaging.function = sentimentr::average_mean) %>%
  highlight()

highlight(sentr_highlight)

# error message:  "Error in `[.data.table`(y, , list(sentiment = attributes(x)[["averaging.function"]](sentiment), : attempt to apply non-function"

#----


```




## Bing Positive/Negative Sentiment Distribution
``` {r}

#bing

#positive/negative sentiment distribution - grouped by show/response
netsent_posneg <- c %>%
  select(Show, Text) %>%
  unnest_tokens(word,Text) %>%
  count(Show,word) %>%
  group_by(Show) %>%
  inner_join(bing,by=c("word")) %>%
  group_by(Show,sentiment) %>%
  summarize(total=sum(n)) %>%
  spread(sentiment,total) %>%
  rename(negative_count = negative, positive_count = positive) %>%
  mutate(netsentiment=positive_count-negative_count, total= negative_count + positive_count, negative = negative_count/total, positive = positive_count/total)

#gather columns
netsent <- netsent_posneg %>%
  gather(key = sentiment_type, value = percentage, c(positive,negative))
netsent

#graph
netsent_barg <- ggplot(netsent, aes(x = Show, y = percentage, fill = sentiment_type)) + geom_bar(stat = 'identity', position = 'dodge') + labs(y = 'Sentiment Percentage', title = 'Sentiment Distribution per Show')

netsent_barg
#-------


#positive/negative sentiment distribution - not grouped/for the whole dataset
netsent_posneg_all <- c %>%
  select(Text) %>%
  unnest_tokens(word,Text) %>%
  count(word) %>%
  inner_join(bing,by=c("word")) %>%
  group_by(sentiment) %>%
  summarize(total=sum(n)) %>%
  spread(sentiment,total) %>%
  rename(negative_count = negative, positive_count = positive) %>%
  mutate(netsentiment=positive_count-negative_count, total= negative_count + positive_count, negative = negative_count/total, positive = positive_count/total)

#gather columns
netsent_all <- netsent_posneg_all %>%
  gather(key = sentiment_type, value = percentage, c(positive,negative))
#netsent_all

#graph
netsent_barg_all <- ggplot(netsent_all, aes(x = sentiment_type, y = percentage, fill = sentiment_type)) + geom_bar(stat = 'identity', position = 'dodge') + labs(y = 'Sentiment Percentage', title = 'Overall Sentiment Distribution')

netsent_barg_all
#-------


```





# Word Counts
``` {r}

#need to remove cirque, du, soleil, las, vegas


# simple word frequencies
#for the original data

#unigrams
unigram_top25 <- c %>%
  unnest_tokens(word,Text) %>%
  count(Show,word) %>%
  anti_join(stop_words) %>%
  group_by(Show) %>%
  top_n(25) %>%
  ggplot(aes(x=reorder_within(word,n,Show),
             y=n,
             fill=Show)) + 
  geom_bar(stat='identity') + 
  coord_flip() +
  scale_x_reordered() +
  facet_wrap(~Show, scales = 'free',nrow=1) + 
  theme_bw() + 
  theme(legend.position = "none")+
  labs(title = 'Top Words by Show',
       subtitle = 'Stop words removed',
       x = 'Word',
       y = 'Count')

unigram_top25

# ---

#bigrams

#need to remove stop words from bigrams
bigram_top25 <- c %>%
  select(Show, Text) %>%
  unnest_tokens(word, Text, token = "ngrams", n = 2) %>%
  count(Show,word) %>%
#  anti_join(stop_words) %>%
  group_by(Show) %>%
  top_n(25) %>%
  ggplot(aes(x=reorder_within(word,n,Show),
             y=n,
             fill=Show)) + 
  geom_bar(stat='identity') + 
  coord_flip() +
  scale_x_reordered() +
  facet_wrap(~Show, scales = 'free',nrow=1) + 
  theme_bw() + 
  theme(legend.position = "none")+
  labs(title = 'Top Bigrams by Show',
       subtitle = 'Stop words removed',
       x = 'Bigram',
       y = 'Count')

bigram_top25


```




